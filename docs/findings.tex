% findings.tex
\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{caption}
\usepackage{geometry}
\geometry{margin=1in}

\title{Autotuning and Self-Referential Dynamics in a Minimal Bistable Workspace} 
\author{Brandon Raeder et al.}
\date{\today}

\begin{document}
\maketitle

\begin{abstract}
We present miniBrain, a compact highly-optimized Python lab (400x speedup) for bistable workspace dynamics with real-time high-FPS animations. Three architectures: (A) global workspace, (B) hierarchical reflective, (C) self-referential predicting own compressed state. Meta-autotuning via neural rollouts biases toward high entropy+coherence. Irrational dual-frequency perturbations prevent lock-in. Metrics: Shannon entropy, Lyapunov proxy, LZ complexity. Optimizations enable smooth interactive visualization.
\end{abstract}

\section{Introduction}
Understanding how global coupling and self-modeling interact with bistable local dynamics is relevant for exploring mechanisms of coherent processing, surprise, and meta-level regulation in minimal neural systems. To this end we developed \texttt{lab.py}, a lightweight and reproducible environment supporting rapid iteration, interactive visualization, automated smoke tests, and parameter autotuning guided by complexity-based reward.

\section{Methods}
\subsection{Model Variants}
We implemented three configurations:
\begin{itemize}
  \item \textbf{Option A}: $N$ bistable units with a global workspace variable $ws$ that integrates mean population activity and feeds back with strength $\epsilon$.
  \item \textbf{Option B}: Two coupled bistable cascades with an additional reflective ``why'' unit driving workspace activity through a saturating map.
  \item \textbf{Option C}: A self-referential workspace maintaining a compressed self-model vector (mean, variance, trend, normalized workspace, short-term entropy, coherence) along with a small predictor that forecasts future self-model values. Prediction error modulates local dynamics.
\end{itemize}

All models use $\tanh(\alpha x - \theta)$ as the bistable activation and Euler integration with timestep $dt$.

\subsection{Autotuning}
A small PyTorch network maps complexity features $[\mathrm{entropy}, R, \mathrm{lyap}, \mathrm{complexity}]$ to normalized parameters $[\hat\alpha, \hat\epsilon]$. Background workers:
\begin{enumerate}
  \item Propose candidate parameters and run short rollouts.
  \item Compute reward: $\mathrm{reward} = w_{H}\cdot H + w_{R}\cdot R_{\text{final}}$.
  \item Store $(\text{features}, \text{params}, \text{reward})$ in an experience buffer and update the meta-tuner by reward-weighted regression.
\end{enumerate}
A fallback heuristic based on entropy is used if PyTorch is unavailable.

\subsection{Irrational-Time Perturbation}
To prevent resonance lock-in, low-amplitude sinusoidal perturbations at two incommensurate periods ($\tau_1, \tau_2$) are introduced with a slow phase drift.

\subsection{Performance Optimizations}
Achieved 100-400x speedup (0.12--0.22\,ms/step, 50 layers/1000 steps):

\begin{itemize}
\item Metric caching + update every 3 frames
\item History buffers: 2000$\to$800 steps
\item Rolling windows: 500$\to$150
\item Bounded/simplified LZ complexity
\item Numba JIT for activations/metrics
\item Efficient circular buffers
\end{itemize}

Enables 10--30 FPS real-time visualization of all models.

\section{Implementation Notes}
The codebase is contained in a single script (\texttt{lab.py}) plus a helper file for self-model utilities.
\begin{itemize}
  \item \textbf{GUI}: A 3Ã—2 layout provides workspace heatmaps and phase traces with rolling-window views.
  \item \textbf{Threading}: Autotuning threads are started only after Matplotlib animations are active to avoid Tk race conditions.
  \item \textbf{Metrics}: Shannon entropy, Lyapunov-step proxy, LZ-76 complexity on thresholded traces, pairwise mutual information, and simple linear decoders.
  \item \textbf{Testing}: \texttt{smoke\_test\_autotune()} validates background worker updates and buffer behavior.
\end{itemize}

\section{Results}
Preliminary runs indicate:
\begin{itemize}
  \item The experience buffer is actively populated and updates the meta-tuner toward higher-entropy regimes.
  \item The dual-frequency perturbation reduces overly persistent oscillatory modes relative to single-frequency perturbation.
  \item Deferring autotune thread initialization until after GUI setup prevents Tk/Tcl instability.
\end{itemize}

\begin{table}[ht]
\centering
\caption{Performance Benchmarks (50 layers, 1000 steps).}
\begin{tabular}{lrrrr}
\toprule
Model & Time (s) & ms/step & FPS & Final $R$ \\
\midrule
Option A & 0.118 & 0.12 & 8471 & 0.579 \\
Option B & 0.130 & 0.13 & 7700 & 0.790 \\
Option C & 0.218 & 0.22 & 4585 & 0.580 \\
\bottomrule
\end{tabular}
\end{table}

\section{Discussion}
Short rollouts combined with a reward-weighted experience buffer provide an effective causal training signal for parameter tuning. Option C introduces a self-modeling signal via prediction error, adding a channel for dynamic self-regulation. Metrics remain proxy-based and require further validation across longer runs and broader parameter sweeps.

\section{Reproducibility}
Dependencies: Python 3.10+, NumPy, Matplotlib, scikit-learn, PyTorch (optional).

\begin{verbatim}
# GUI (interactive)
python3 lab.py

# Headless smoke test
python3 -c "import matplotlib; matplotlib.use('Agg'); \
             import lab; lab.smoke_test_autotune(2.0)"
\end{verbatim}

\section{Conclusion}
This environment offers a compact platform for exploring workspace-mediated dynamics, self-modeling, and automatic parameter adjustment. Its minimal footprint is intended to encourage experimentation and modification.

\section*{Acknowledgements}
Development and testing were performed by the project team. Code is available at:
\texttt{https://github.com/BrandonRaeder/miniBrain}

\begin{thebibliography}{9}
\bibitem{tononi2008}
G. Tononi, \emph{Consciousness as Integrated Information}, Biological Bulletin, 2008.

\bibitem{wolpert1995}
D. Wolpert, Z. Ghahramani, M. Jordan, \emph{An internal model for sensorimotor integration}, 1995.
\end{thebibliography}

\end{document}
